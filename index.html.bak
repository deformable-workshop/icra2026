<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<script>
// Set the date we're counting down to
var countDownDate = new Date("2025-05-23T08:30:00.000-04:00");
// Update the count down every 1 second
var x = setInterval(function() {

  // Get today's date and time
  var now = new Date().getTime();

  // Find the distance between now and the count down date
  var distance = countDownDate - now;

  // Time calculations for days, hours, minutes and seconds
  var days = Math.floor(distance / (1000 * 60 * 60 * 24));
  var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
  var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60));
  var seconds = Math.floor((distance % (1000 * 60)) / 1000);

  // Display the result in the element with id="demo"
  document.getElementById("demo").innerHTML = days + "d " + hours + "h "
  + minutes + "m " + seconds + "s ";

  // If the count down is finished, write some text
  if (distance < 0) {
    clearInterval(x);
    document.getElementById("demo").innerHTML = "It's LIVE";
  }
}, 1000);
</script>


<html>
	<head>
		<title>DOM Workshop ICRA 2026</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<!-- table style-->
		<style>
			a{
				color: white;
			}
			#schedule_tab {
			  font-family: Arial, Helvetica, sans-serif;
			  border-collapse: collapse;
			  width: 100%;
			}

			#schedule_tab td, #schedule_tab th {
			  border: 2px solid #ddd;
			  padding: 14px;
			  color: white;
			}

			#schedule_tab tr:nth-child(even){background-color: #9fabbd;}

			#schedule_tab tr:hover {background-color: #a8b9d2;}

			#schedule_tab th {
			  padding-top: 12px;
			  padding-bottom: 12px;
			  text-align: left;
			  background-color: #374e73;
			  color: white;
			}
		</style>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<!-- <li><a href="#live">Live</a></li> -->
							<li><a href="#intro">Home</a></li>
							<li><a href="#content">Content</a></li>
							<li><a href="#challenge">Challenge</a></li>
							<li><a href="#schedule">Schedule</a></li>
							<li><a href="#papers">Call for Papers</a></li>

							<li><a href="#talks">Invited speakers</a></li>
							<li><a href="#org">Organizers</a></li>
							<!--li><a href="#collab">Links</a></li-->
							<li><a href="#contact">Contact</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Live -->
					<!-- <section id="live" class="wrapper style3 fullscreen fade-up">
						<div class="inner">
							<h2>6th Workshop on Deformable Objects Manipulation: Research Foundations, Reproducibility and Real-World Challenges @ <a href="https://2026.ieee-icra.org/" target="_blank">ICRA2026</a> </h2>

							<iframe width="1120" height="630" src="https://www.youtube.com/embed/da0U3lamgLM?si=xNAaXauA_ovlv6Yg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<h3/>
<h3> Slido event for interaction: TBD </h3>
<h3>Meeting Room: TBD </h3>
						</div>
						
					</section> -->

<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">

							<!--h1 id="demo"></h1-->

							<h2>6th Workshop on Deformable Objects Manipulation: Research Foundations, Reproducibility and Real-World Challenges @ <a href="https://2026.ieee-icra.org/" target="_blank">ICRA2026</a> </h2>

							

							<p>
							Deformable object manipulation, spanning garments, soft foods, flexible packaging, cables, ropes, and tissues, has rapidly become a cornerstone testbed for embodied intelligence. Despite impressive demonstrations and steady academic advances, the field still lacks clear, comparable evidence of the progress and real-world capabilities due to gaps in reproducibility and benchmarking. 
This workshop renews the RMDO@ICRA series with the intent of uniting academic contributions with a renewed focus on system-level challenges. The research track welcomes contributions addressing cutting-edge research topics such as state representation, simulation and modeling, perception, and large-scale data-driven approaches, including recent advances in vision-language-action (VLA) models. The system-level track comprises (i) synergies with a Garment Manipulation Challenge that includes simulation and real-world tasks, and (ii) organizer-led reproductions of influential methods for real-world tasks such as folding and flattening, conducted in collaboration with original authors to surface design choices and practical constraints.
Our goal is not to re-rank methods, but to identify how hard it is to reproduce different methods, what the challenges are in doing so and how well these methods perform under diverse conditions, using transparent metrics for success, generalization, sample efficiency, robustness, and versatility. By integrating principled research with comparative, open evaluation across deformable classes, the workshop will provide a calibrated view of current capabilities and limitations, and highlight the methodological and system-level gaps for solving real-world tasks in deformable object manipulation.
</p>



							<h3> The workshop will be held in <b>hybrid mode</b></h3>
							<!--h3> The workshop was  held in <b>hybrid mode</b> </h3-->

							<h3> Links: </h3>
							<ul id="link_list">
							<!--li> InfoVaya: <a href="https://events.infovaya.com/event?id=138" target="_blank">https://events.infovaya.com/event?id=138</a>  </li-->
							<li> YouTube channel: <a href="https://www.youtube.com/channel/UCQFAnfbQK45enYDr8B0VdKw" target="_blank">https://www.youtube.com/channel/UCQFAnfbQK45enYDr8B0VdKw</a>  </li-->
							<li>Link to 5th edition of the workshop: <a href="https://deformable-workshop.github.io/icra2025/" target="_blank">https://deformable-workshop.github.io/icra2025/</a></li>
							<li>Link to 4th edition of the workshop: <a href="https://deformable-workshop.github.io/icra2024/" target="_blank">https://deformable-workshop.github.io/icra2024/</a></li>
							<li>Link to 3rd edition of the workshop: <a href="https://deformable-workshop.github.io/icra2023/" target="_blank">https://deformable-workshop.github.io/icra2023/</a></li>
							<li>Link to 2nd edition of the workshop: <a href="https://deformable-workshop.github.io/icra2022/" target="_blank">https://deformable-workshop.github.io/icra2022/</a></li>
							<li>Link to 1st edition of the workshop: <a href="https://deformable-workshop.github.io/icra2021/" target="_blank">https://deformable-workshop.github.io/icra2021/</a></li>

							</ul>




						</div>
					</section>




					<!-- Method -->
					<section id="content" class="wrapper style2 fade-up">
						<div class="inner">
							<h2>Content</h2>
							<h3>Topics</h3>

							<p>
								Over the past five years, the RMDO@ICRA workshop series has established itself as the central venue for discussing progress, challenges, and opportunities in the very active field of deformable object manipulation. Building on this foundation and acknowledging the advances in deformable object manipulation, this workshop introduces a renewed perspective that extends the scope of the workshop to explicitly include system-level challenges for real-world deformable manipulation, including evaluation and reproducibility. To foster interaction about these system-level challenges, we will complement contributed research efforts on deformable object manipulation with two distinctive components 
							<ul>
  <li>(i) a Garment Manipulation Challenge spanning simulation and real-world tasks, </li>
<li>  (ii) organizer-led reproductions of influential methods for folding and flattening, conducted in collaboration with original authors. </li>
</ul>
At the same time, the workshop will remain a forum for cutting-edge academic research across the spectrum of deformable manipulation, including garments, flexible packaging, soft foods, and tissues. The workshop reflects the state of the art by engaging with emerging research frontiers, including the integration of vision–language–action (VLA) and foundation models, reinforcement and imitation learning, dexterous and bimanual control, and scalable 3D simulation. 
The academic focus will span the breadth of deformable object manipulation, including:
<ul>
  <li>Representation and state estimation</li>
  <li> Simulation and modeling </li>
  <li>Transfer from simulation to reality</li>
  <li>Learning to manipulate using data-driven methods such as reinforcement learning and learning from demonstrations </li>
  <li>Perception: state tracking, parameter identification, property detection (e.g. landmarks for
garments) and classification, etc. </li>
  <li>Control, visual servoing and planning</li>
  <li> Use of foundation models, such as large vision and language models, and associated large datasets</li>
  <li>Specialized tools, e.g. grippers, and sensors</li>


							<p> </p>

							<h3>Workshop format </h3>
						<p>
						The workshop will include:
<ul><li>Invited talks by selected speakers, each consisting of about 20 minutes of presentation and 5 minutes for Q&A;<\li>
<li>Follow-up Reproducibility Discussions, consisting of a 15-minute dialogue between the invited speaker and the co-organizer who reproduced their paper. This session will highlight practical challenges, clarifications provided by the original authors, and broader lessons for reproducibility in robotics research.</li>
<li>Accepted extended abstracts (3 pages with unlimited references and appendix) presented in poster sessions and selected spotlight talks. In case of a hybrid or virtual workshop, we will ask for pre-recorded spotlight talks for a smoother execution in case of connection issues. However, for each selected contribution, at least one author will be required to be present during the workshop for a live Q&A session;  </li>
<li>Competition Spotlights, where winners of the simulation and real-world tracks of the Garment Manipulation Challenge will present “lessons learned,” focusing on design choices, challenges, and successes in perception, modeling, and control.
</li>
<li>A panel discussion at the end of the workshop, moderated by the organizers,  for discussing challenges and promising directions for deformable object manipulation with experts of the field. 
	</p>

						</div>
					</section>

<!-- Method -->
					<section id="challenge" class="wrapper style4 fullscreen fade-up">
						<div class="inner">
							<h2>Garment Manipulation Challenge</h2>
							TBD
						</div>
					</section>		
				<!-- Schedule -->
					<section id="schedule" class="wrapper style3 spotlights">
						<div class="inner">
							<h2>Schedule  <!-- a href="schedule_poster_panel.pdf" target="_blank">[PDF]</a--></h2>
							<hr style="width:80%;text-align:left;margin-left:5">
							<div class="row">

									<p></p>
							TBD
							</div>


							<!-- <h3>Time Zone: GMT -04</h3>

<table id="schedule_tab">

  <tr>
    <th>Time</th>
    <th>Activity</th>
  </tr>

  <tr>
    <td>08:30 - 08:45</td>
    <td>Workshop opening</td>
  </tr>

  <tr>
    <td>08:45 - 09:15</td>
    <td><b>Jeannette Bohg</b> [In person] - Object-centric or not: How to represent Deformables for Manipulation?</td>
  </tr>

  <tr>
    <td>09:15 - 10:00</td>
    <td>
      Spotlight Talks #1 
      <ul>
		<li>Mahdiar Edraki*; Silvia Buscaglione; Rakshith Lokesh; John Peter Whitney; Alireza Ramezani; Dagmar Sternad; <b>Human-Inspired Robot Whip Manipulation: Preparatory Actions Increase Range and Reduce Control Effort <a href="spotlight/01_01_05_Edraki_Human.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/e08K-8K7hFE" target="_blank">[Video]</a></b></li>
<li>Wendi Chen*; Han Xue; Fangyuan Zhou; Yuan Fang; Cewu Lu; <b>DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment <a href="spotlight/01_02_10_Chen_DeformPAM.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/0zYwjLtcpQk" target="_blank">[Video]</a></b></li>
<li>Haodi Hu*; Yue Wu; Tian Xie; Daniel Seita; Feifei Qian; <b>Granular loco-manipulation: Repositioning rocks through strategic sand avalanche <a href="spotlight/01_03_11_Hu_granular.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/KJk6iJ7duoE" target="_blank">[Video]</a></b></li>
<li>Oriol Barbany*; Adrià Colomé; Carme Torras; <b>Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding <a href="spotlight/01_04_13_Barbany_beyond.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/5jef2jdInvU" target="_blank">[Video]</a></b></li>
<li>Jay Kamat*; Júlia Borràs; Carme Torras; <b>A compact cloth state representation that enables generalization across different cloth shapes <a href="spotlight/01_05_14_Kamat_compact.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/sNZBkpTk85Y" target="_blank">[Video]</a></b></li>
<li>Holly Dinkel*; Marcel Büsching; Alberta Longhini; Brian Coltin; Trey Smith; Danica Kragic; Mårten Björkman; Timothy Bretl; <b>DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting <a href="spotlight/01_06_15_Dinkel_DLO-Splatting.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/CG4WDWumGXA" target="_blank">[Video]</a></b></li>
<li>Yuhong Deng*; David Hsu; <b> General-purpose Clothes Manipulation with Semantic Keypoints <a href="spotlight/01_07_17_Deng_general.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/oc_r4mdx_cc" target="_blank">[Video]</a></b></li>
<li>Mahdi Bonyani*; Maryam Soleymani; Chao Wang; <b>Robust Shape-Free Manipulation through a Graph-Based Reinforcement Learning Approach for Deformable Objects <a href="spotlight/01_08_18_Bonyani_robust.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/hUULJDIh44Q" target="_blank">[Video]</a></b></li>
      </ul>
    </td>
  </tr>

  <tr>
    <td>10:00 - 11:00</td>
    <td>Coffee Break + Poster Presentation</td>
  </tr>

  <tr>
    <td>11:00 - 11:30</td>
    <td><b>Yiannis Demiris</b> [Remote] - Manipulation of Deformable Objects in Assistive Tasks</td>
  </tr>

  <tr>
    <td>11:30 - 12:00</td>    
    <td><b>Shuran Song</b> [In person] - Five Lessons Learned on Deformable Object Manipulation</td>
  </tr>

  <tr>
    <td>12:00 - 12:30</td>    	
    <td><b>Ken Goldberg</b> [In person] - AI + GOFE for Manipulating 1D, 2D, and 3D Deformable Objects</td>
  </tr>

  <tr>
    <td>12:30 - 13:30</td>
    <td>Lunch Break</td>
  </tr>

  <tr>
    <td>13:30 - 14:00</td>
    <td><b>Zackory Erickson</b> [Remote] - Physical HRI with Deformable Manipulation</td>
  </tr>

  <tr>
    <td>14:00 - 14:30</td>
    <td><b>Chuang Gan</b> [In person] - Genesis: A Generative and Universal Physics Engine for Robotics</td>
  </tr>

  <tr>
    <td>14:30 - 15:15</td>
    <td>
      Spotlight Talks #2 
      <ul>
        <li>Hanxiao Jiang*; Hao-Yu Hsu; Kaifeng Zhang; Hsin-Ni Yu; Shenlong Wang; Yunzhu Li; <b>PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos <a href="spotlight/02_01_03_Jiang_PhysTwin.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/j5XLZepGMno" target="_blank">[Video]</a></b></li>
<li>Kavish Kondap*; Osher Azulay; Jaimyn Drake; Shuangyu Xie; Hui Li; Sachin Chitta; Ken Goldberg; <b>Adaptive Cable Manipulation Around Constrained Fixtures with Bi-Manual Slack Control <a href="spotlight/02_02_04_Kondap_adaptive.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/MpmCXLxb_k4" target="_blank">[Video]</a></b></li>
<li>Andreas Mueller*; <b>Analytic Continuum Forward Kinematics of Deformable Linear Objects under Static Conditions <a href="spotlight/02_03_07_Mueller_analytic.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/cw4dqlJoqLo" target="_blank">[Video]</a></b></li>
<li>Nikhil Shinde*; Xiao Liang; Fei Liu; Yutong Zhang; Florian Richter; Sylvia Herbert; Michael Yip; <b>JIGGLE: An Active Sensing Framework for Boundary Parameters Estimation in Deformable Surgical Environments <a href="spotlight/02_04_08_Shinde_JIGGLE.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/pLAgEewAUL0" target="_blank">[Video]</a></b></li>
<li>Alessio Caporali*; Gianluca Palli;<b> Multi-View Model-Based Visual Tracking of Deformable Linear Objects  <a href="spotlight/02_05_09_Caporali_multi.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/DAoNi7xFxTk" target="_blank">[Video]</a></b></li>
<li>Zhaole Sun*; <b>Dexterous Cable Manipulation: An Initial Exploration <a href="spotlight/02_06_12_Sun_dexterous.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/ASfQpc0gG6c" target="_blank">[Video]</a></b></li>
<li>Nidhya Shivakumar*; Justin Yu; Veena Sumedh; Josh Zhang; Ethan Ransing; Osher Azulay; Ken Goldberg; <b>HANDLOOM 3.0: Interactive Bi-Directional Cable Tracing Amid Clutter <a href="spotlight/02_07_16_Shivakumar_HANDLOOM.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/GJJB4h1eWbU" target="_blank">[Video]</a></b></li>
<li>Simeon Adebola*; Chung Min Kim; Justin Kerr; Shuangyu Xie; Prithvi Akella; Jose Luis Susa Rincon; Eugen Solowjow; Ken Goldberg; <b>A “Botany-Bot” for Digital Twin Monitoring of Occluded and Underleaf Plant Structures <a href="spotlight/02_08_19_Adebola_botany-bot.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/Lq9jlzFiHuQ" target="_blank">[Video]</a></b></li>
 <li>Kaifeng Zhang*; Baoyu Li; Kris Hauser; Yunzhu Li; <b>Particle-Grid Neural Dynamics for Learning Deformable Object Models from Depth Images <a href="spotlight/02_09_20_Particle_Grid_Neural_Dynamics.pdf" target="_blank">[PDF]</a>  <a href="https://youtu.be/mEvCpUoUvLY" target="_blank">[Video]</a></b></li>


 </ul>
    </td>
  </tr>

  <tr>
    <td>15:15 - 16:00</td>
    <td>Coffee Break + Poster Presentation</td>
  </tr>

  <tr>
    <td>16:00 - 16:30</td>
    <td><b>Yunzhu Li</b> [In person] - Learning Structured World Models From and For Physical Interactions</td>
    
  </tr>

  <tr>
    <td>16:30 - 17:00</td>
    <td><b>Panel Discussion</b> - Reflections & Future Directions in Deformable Object Manipulation</td>
  </tr>

  <tr>
    <td>17:00 - 17:30</td>
    <td>Best Abstract Award & Closing Remarks</td>
  </tr>

</table>
 -->

					


					</section>






















						<section id="papers" class="wrapper style4 fullscreen fade-up">
    <div class="inner">
        <h2>Call for Papers</h2>

        <p>We invite participants to submit extended abstracts of <b>3 pages</b>, with unlimited pages for references and appendices, in the <a href="https://journals.ieeeauthorcenter.ieee.org">IEEE conference style</a>. Submissions will be reviewed by experts in their respective fields. The accepted abstracts will be made available on the workshop website but will not appear in the official IEEE conference proceedings. Participants are encouraged to submit their recent work on the topics of interest mentioned above.</p>

        <p>Contributions are encouraged, but are not required, to be original. The review process will be single-blind, meaning the submitted paper does not need to be anonymized.</p>

        <p>Accepted extended abstracts will be presented in poster sessions and selected spotlight talks. We will request pre-recorded spotlight talks to ensure a smoother execution of the workshop. However, for each selected contribution, at least one author will be required to be present during the workshop for a live Q&A session.</p>

        <p>Abstracts can be submitted through Microsoft CMT: <a href="https://cmt3.research.microsoft.com/WDOICRA2025/" target="_blank">WDOICRA2025</a>.</p>

        <p> </p>
							<!-- <h3>IEEE RAS Computer & Robot Vision workshop award</h3>
							<p>We are happy to announce the <b>WDO Best Abstract Award </b> sponsored by the <a href="https://www.ieee-ras.org/computer-robot-vision" target="_blank">IEEE RAS Technical Committee Computer & Robot Vision</a>.
							The selected contribution will receive a <b> prize of worth 300$</b>.
							Any extended abstract or reflection submitted to the workshop will be automatically considered for the award.</p>
 -->
        <h3>Important Dates: (dd/mm/yyyy)</h3>
        <ul>
            <!-- <li>Submission Deadline: <s>31/03/2025</s>  <s>07/04/2025</s> <b>14/04/2025 (firm deadline)</b> (23:59 PST)</li>
            <li>Notification Date:   <s>23/04/2025</s> <b>30/04/2025</b>  (23:59 PST)</li>
            <li>Final Submission:  <s>30/04/2025</s> <b>07/05/2025</b> (23:59 PST)</li>
            <li>Workshop Date: <b>23/05/2025</b></li> -->
			<li>Submission Deadline: TBD
            <li>Notification Date:   TBD
            <li>Final Submission:  TBD
            <li>Workshop Date: TBD
        </ul>
    </div>
</section>



					<!-- call for papers -->
					<!--section id="papers" class="wrapper style4 fullscreen fade-up">
						<div class="inner">
							<h2>Call for papers</h2>


							<p> We invite participants to submit extended abstracts <b>3+n</b> pages, with n pages (no page-limit) for the bibliography, in the <a href="https://journals.ieeeauthorcenter.ieee.org">IEEE conference style</a>. </p>
							<p> Submissions will be reviewed by experts of their respective field. The accepted abstracts will be made available on the workshop website but will not appear in the official IEEE conference proceedings.
							 Participants are encouraged to submit their recent work on the topics of interest mentioned above.
							<contributions that highlight challenges in their particular sub-field as well as works that show potential synergies of combining different subfields for deformable objects manipulation. >
							Contributions are encouraged, but are not required, to be original. </p>
							<p> The review process will be single-blind, meaning the submitted paper does not need to be anonymized.</p>

							<p> Abstracts can be submitted through Microsoft CMT: TBA </p>  <a href="https://cmt3.research.microsoft.com/WDOICRA2025">https://cmt3.research.microsoft.com/WDOICRA2025</a>. </p
					
							<p> </p>
							<h3>IEEE RAS Computer & Robot Vision workshop award</h3>
							<p>We are happy to announce the <b>WDO Best Abstract Award </b> sponsored by the <a href="https://www.ieee-ras.org/computer-robot-vision" target="_blank">IEEE RAS Technical Committee Computer & Robot Vision</a>.
							The selected contribution will receive a <b> prize of 400$</b>.
							Any extended abstract submitted to the workshop will be automatically considered for the award.</p>

							<p> <h3>Important dates: (dd/mm/yyyy) </h3> </p>

							<ul>
								  <li>Submission Deadline: <b>31/03/2025</b> (23:59 PST)  </li>
								  <li>Notification date: <b>16/04/2025 </b> (23:59 PST) </li>
								  <li>Final submission: <b>23/04/2025 </b> (23:59 PST)</li>

								  <li>Workshop date: <b> 23/05/2025 </b>
 								</li>
							</ul>

						
						</div>
					</section-->



					<!-- Schedule -->
					<section id="talks" class="wrapper style2 spotlights">
						<div class="inner">
							<h2>Invited Speakers (alphabetical order): </h2>
							TBA
							<!-- <div class="row">
							  <div class="column">
							    <img src="imgs/jeannette.png" alt="Jeannette Bohg" class="img-list">
							  </div>
							  <div class="column" >
									<p></p>
				    				<h3 class='name-list'><b>  Jeannette Bohg </b></h3>

				    				<div class='info-list'>
									<br>
									Assistant Professor
									<br>
									Stanford University, USA
									<br>
									<a href="https://web.stanford.edu/~bohg/" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> Object-centric or not: How to represent Deformables for Manipulation?
 <br>
								</p>							

							  </div>
							</div>
							<p>
							<b> Bio:</b> Jeannette Bohg is an Assistant Professor of Computer Science at Stanford University. She was a group leader at the Autonomous Motion Department (AMD) of the MPI for Intelligent Systems until September 2017. Before joining AMD in January 2012, Jeannette Bohg was a PhD student at the Division of Robotics, Perception and Learning (RPL) at KTH in Stockholm. In her thesis, she proposed novel methods towards multi-modal scene understanding for robotic grasping. She also studied at Chalmers in Gothenburg and at the Technical University in Dresden where she received her Master in Art and Technology and her Diploma in Computer Science, respectively. Her research focuses on perception and learning for autonomous robotic manipulation and grasping. She is specifically interested in developing methods that are goal-directed, real-time and multi-modal such that they can provide meaningful feedback for execution and learning. Jeannette Bohg has received several Early Career and Best Paper awards, most notably the 2019 IEEE Robotics and Automation Society Early Career Award and the 2020 Robotics: Science and Systems Early Career Award. <br>
									</p>

							<hr style="width:80%;text-align:left;margin-left:5">


							<div class="row">
							  <div class="column">
							    <img src="imgs/Yiannis.jpeg" alt="Yiannis Demiris" class="img-list">
							  </div>
							  <div class="column">
									<p></p>
				    				<h3 class='name-list'><b>  Yiannis Demiris </b></h3>

				    				<div class='info-list'>
									<br>
									 Professor
									<br>
									Imperial College London, UK
									<br>
									<a href="https://profiles.imperial.ac.uk/y.demiris" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> Manipulation of deformable objects in assistive tasks <br>
								</p>							

							  </div>
							</div>
							<p>
							<b> Bio:</b> Yiannis Demiris is a professor of human-centered robotics at Imperial College London, where he leads the Personal Robotics Laboratory, supported by a Royal Academy of Engineering Chair in Emerging Technologies. His research interests include human-centered multimodal perception, user modelling, and collaborative human-robot control, with a special emphasis on their application to assistive robotics. Prior to joining Imperial, he received his PhD in Intelligent Robotics and his BSc in Artificial Intelligence and Computer Science, both from the University of Edinburgh. <br>
									</p>

							<hr style="width:80%;text-align:left;margin-left:5">


							<div class="row">
							  <div class="column">
							    <img src="imgs/Zackory.jpg" alt="Zackory Erickson" class="img-list">
							  </div>
							  <div class="column">
									<p></p>
				    				<h3 class='name-list'><b>  Zackory Erickson </b></h3>

				    				<div class='info-list'>
									<br>
									Assistant Professor
									<br>
									 Carnegie Mellon University (CMU), USA
									<br>
									<a href="https://zackory.com/" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> Physical HRI with Deformable Manipulation <br>
								</p>							

							  </div>
							</div>
							<p>
							<b> Bio:</b> Zackory Erickson is an Assistant Professor in The Robotics Institute at Carnegie Mellon University, where he leads the Robotic Caregiving and Human Interaction (RCHI) Lab. His research focuses on developing new robot learning, mobile manipulation, and sensing methods for physical human-robot interaction and healthcare. Zackory’s work spans physical human-robot interaction, healthcare robotics, wearable health sensing, robot learning, physics simulation, multimodal perception, and mobile manipulation. Prior to joining CMU, Zackory received his PhD in Robotics from Georgia Tech with Prof. Charlie Kemp. He also received an M.S. in Computer Science from Georgia Tech and B.S. in Computer Science at the University of Wisconsin–La Crosse. He and his students have received Best Paper Award at HRI 2024, Best Student Paper Award at ICORR 2019, and a Best Paper in Service Robotics finalist at ICRA 2019. <br>
									</p>

							<hr style="width:80%;text-align:left;margin-left:5">


							<div class="row">
							  <div class="column">
							    <img src="imgs/Chuan.jpg" alt="Chuang Gan" class="img-list">
							  </div>
							  <div class="column">
									<p></p>
				    				<h3 class='name-list'><b>  Chuang Gan</b></h3>

				    				<div class='info-list'>
									<br>
									Assistant Professor
									<br>
									UMass Amherst, USA
									<br>
									<a href="https://mitibmwatsonailab.mit.edu/people/chuang-gan/" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> Genesis: A Generative and Universal Physics Engine for Robotics <br>
								</p>							

							  </div>
							</div>
							<p>
								
							<b> Bio:</b> Chuang Gan is a faculty member at UMass Amherst and a research manager at the MIT-IBM Watson AI Lab. Previously, Chuang Gan was a postdoctoral researcher at MIT, working with Professors Antonio Torralba, Daniela Rus, and Josh Tenenbaum. Before that, Chuang Gan completed a PhD with the highest honor at Tsinghua University under the supervision of Professor Andrew Chi-Chih Yao.

							Chuang Gan's research lies at the intersection of computer vision, AI, cognitive science, and robotics. The overarching goal of this research is to develop human-like autonomous agents capable of sensing, reasoning, and acting in the physical world. Chuang Gan's work has been recognized with the Microsoft Fellowship and Baidu Fellowship and has received media coverage from CNN, BBC, The New York Times, WIRED, Forbes, and MIT Technology Review.		</p>

							<hr style="width:80%;text-align:left;margin-left:5">


							<div class="row">
							  <div class="column">
							    <img src="imgs/Ken_Goldberg.jpg" alt="Ken Goldberg" class="img-list">
							  </div>
							  <div class="column">
									<p></p>
				    				<h3 class='name-list'><b>  Ken Goldberg</b></h3>

				    				<div class='info-list'>
									<br>
									Professor, William S. Floyd Jr. Distinguished Chair in Engineering 
									<br>
									UC Berkeley, USA
									<br>
									<a href="http://goldberg.berkeley.edu" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> AI + GOFE for Manipulating 1D, 2D, and 3D Deformable Objects <br>
								</p>							

							  </div>
							</div>
							<p>
								
							<b> Bio:</b> Ken Goldberg (IEEE Fellow, 2005) is President of the Robot Learning Foundation
													and William S. Floyd Distinguished
													Chair of Engineering at UC Berkeley and Chief Scientist of Ambi
													Robotics and Jacobi Robotics. Ken leads research in robotics and
													automation: grasping, manipulation, and learning for applications in
													warehouses, industry, homes, agriculture, and robot-assisted surgery.
													He is Professor of IEOR with appointments in EECS and Art Practice.
													Ken is Chair of the Berkeley AI Research (BAIR) Steering Committee (60
													faculty) and is co-founder and Editor-in-Chief emeritus of the IEEE
													Transactions on Automation Science and Engineering (T-ASE). He has
													published ten US patents and over 400 refereed papers. He has
													presented over 600 invited lectures to academic and corporate
													audiences. 	</p>

							<hr style="width:80%;text-align:left;margin-left:5">




							<div class="row">
							  <div class="column">
							    <img src="imgs/yunzhu.jpg" alt="Yunzhu Li" class="img-list">
							  </div>
							  <div class="column">
									<p></p>
				    				<h3 class='name-list'><b>  Yunzhu Li</b></h3>

				    				<div class='info-list'>
									<br>
									 Assistant Professor
									<br>
									Columbia University, USA
									<br>
									<a href="https://yunzhuli.github.io/" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> Learning Structured World Models From and For Physical Interactions<br>
								</p>							

							  </div>
							</div>
							<p>
							<b> Bio:</b> Yunzhu Li is an Assistant Professor of Computer Science at Columbia University. Before joining Columbia, he was an Assistant Professor at UIUC CS, spent time as a Postdoc at Stanford, and earned his PhD from MIT. Yunzhu’s work has been recognized with the Best Paper Award at ICRA, the Best Systems Paper Award, and as a Finalist for the Best Paper Award at CoRL. Yunzhu is also the recipient of the AAAI New Faculty Highlights, the Sony Faculty Innovation Award, the Adobe Research Fellowship, and was selected as the First Place Recipient of the Ernst A. Guillemin Master’s Thesis Award in AI and Decision Making at MIT. His research has been published in top journals and conferences, including Nature and Science, and has been featured by major media outlets. <br>
									</p>

							<hr style="width:80%;text-align:left;margin-left:5">

							<div class="row">
							  <div class="column">
							    <img src="imgs/shuran_song.jpg" alt="Shuran Song" class="img-list">
							  </div>
							  <div class="column">
									<p></p>
				    				<h3 class='name-list'><b> Shuran Song</b></h3>

				    				<div class='info-list'>
									<br>
									Assistant Professor
									<br>
									Stanford University, USA
									<br>
									<a href="https://shurans.github.io/" target="_blank">Personal website</a>
									<br>
									</div>
									<p class='talk-list'>
									<b> Talk title:</b> Five Lessons Learned on Deformable Object Manipulation<br>
								</p>							

							  </div>
							</div>
							<p>
							<b> Bio:</b> Shuran Song is an Assistant Professor of Electrical Engineering at Stanford University. Before joining Stanford, she was faculty at Columbia University. Shuran received her Ph.D. in Computer Science at Princeton University, BEng. at HKUST. Her research interests lie at the intersection of computer vision and robotics. Song’s research has been recognized through several awards, including the Best Paper Awards at RSS’22 and T-RO’20, Best System Paper Awards at CoRL’21, RSS’19, and finalists at RSS, ICRA, CVPR, and IROS. She is also a recipient of the NSF Career Award, Sloan Foundation fellowship as well as research awards from Microsoft, Toyota Research, Google, Amazon, and JP Morgan. <br>
									</p>

							<hr style="width:80%;text-align:left;margin-left:5">
 -->


							<!--ul>
								<li>Jeannette Bohg, Associate Professor, Assistant University, USA</li>
								<li>Yiannis Demiris, Professor, Imperial College London, UK</li>
								<li>Zackory Erickson, AAssistant Professor, Carnegie Mellon University, USA</li>
								<li>Chuang Gan, Assistant Professor, UMass Amherst, USA</li>
								<li>Yunzhu Li, Assistant Professor, Columbia University, USA</li>
								<li>Shuran Song, Assistant Professor, Stanford University, USA</li>
							</ul-->

							<hr style="width:80%;text-align:left;margin-left:5">




					</section>

					<!-- org -->
					<section id="org" class="wrapper style3 spotlights">
						<div class="inner">
							<h2>Organizers </h2>
							<ul>
							  <li>Alberta Longhini, KTH Royal Institute of Technology, Sweden</li>
							  <li>Ruihai Wu, Peking University and Berkeley, China</li>
							  <li>Yuran Wang, Peking University, China</li>
							  <li>Marco Moletta,  KTH Royal Institute of Technology, Sweden</li>
							  <li>Tara Sadjapour, Berkeley, USA </li>
							  <li>Martina Lippi, Roma Tre University, Italy</li>
							  <li>Júlia Borràs Sol, Institut de Robòtica i Informàtica Industrial (CSIC-UPC), Spain</li>
							  <li>Andrej Gams, Jožef Stefan Institute, Slovenia</li>
							  <li>Danica Kragic, KTH Royal Institute of Technology, Sweden</li>
							  <li>Thomas Lips, Ghent University, Belgium</li>
							  <li>Francis Wyffels, Ghent University, Belgium</li>
							</ul>
							
							
					</section>

					<!-- collaberations -->
					<!--section id="collab" class="wrapper style4 fade-up">
						<div class="inner">
							<h2>Links</h2>
							<ul>
							<li>
							  <b>Conference website:</b> Link to the conference website <a href="https://www.ieee-icra.org/">https://www.ieee-icra.org/</a>
							</li>
							</ul>
						</div>
					</section-->




					<!-- contact -->
					<section id="contact" class="wrapper style1 spotlights">
						<div class="inner">
							<h2>Contact</h2>
							 <p> If you have any questions please contact Alberta Longhini at the email: <b>albertal AT kth DOT se </b>  </p>

							 <!-- <h2> Acknowledgment </h2> 
							 <p> The <a href="https://cmt3.research.microsoft.com/Conference/Recent" target="_blank">Microsoft CMT service </a> was used for managing the peer-reviewing process for this conference. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support. </p>-->
						</div>
					</section>




			</div>

		<!-- Footer -->
		<!-- TBD -->

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
